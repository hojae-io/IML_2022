{"backend_state":"init","kernel":"python3","kernel_state":"idle","kernel_usage":{"cpu":0,"memory":0},"metadata":{"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.12"}},"trust":false,"type":"settings"}
{"cell_type":"code","id":"129e0e","input":"##TODO create a new column with tokens in lowercase (x.lower()), without punctuation tokens (x.is_punct) nor stopwords (x.is_stop)\n\ndef tokenize(x):\n    return [w.lemma_.lower() for w in nlp(x) if not w.is_stop and not w.is_punct and not w.is_digit]\ndfs[\"preprocessed\"] = dfs[\"text\"].apply(lambda x: tokenize(x))\n\n##TODO print the tokens (x.lemma_) and the tags (x.tag_ ) of the first sentence of the first document (doc.sents)\nfor sent in dfs.iloc[0][\"tokenized\"].sents:\n    for token in sent:\n        print (token.lemma_, token.tag_)\n    break","pos":5,"type":"cell"}
{"cell_type":"code","id":"4081ac","input":"from sklearn.feature_extraction.text import HashingVectorizer\n\nhv = HashingVectorizer(n_features=5000)\n\n##TODO print the first 10 features produced by the hash vectorizer\nfrom eli5.sklearn import InvertableHashingVectorizer\nX_hash = hv.fit_transform(dfs['input_TFIDF'])\nivec = InvertableHashingVectorizer(hv)\ninverted_hv = ivec.fit(dfs['input_TFIDF'])\n\n#print ([i for i in inverted_hv.get_feature_names()[:10]])\ncounter = 1\nfor i in inverted_hv.get_feature_names():\n    if counter > 10:\n        break\n    if isinstance(i, list):\n        print (i)\n        counter += 1\n        ","pos":17,"type":"cell"}
{"cell_type":"code","id":"4c2a2e","input":"##TODO print the ratio of capitalized tokens not being part of a named entity span\n# e.g. \"The dog barks\" = 1/3; 3 tokens, only \"The\" is capitalized\n\nnum_capitalized, n = 0, 0\n\nfor doc in dfs[\"tokenized\"]:\n    for token in doc:\n        if not token.ent_type_:\n            if token.text[0].isupper():\n                num_capitalized += 1\n            n += 1\nprint (num_capitalized / n)","pos":10,"type":"cell"}
{"cell_type":"code","id":"7237d2","input":"import spacy\ndfs = df.sample(50)\nnlp = spacy.load('en_core_web_sm')\n\n##TODO use spacy to split the documents in the sampled dataframe (dfs) in sentences and tokens\n\ndfs[\"tokenized\"] = dfs[\"text\"].apply(lambda x: nlp(x))\n\n##TODO print the first sentence of the first document in your sample\n\nprint (list(dfs.iloc[0][\"tokenized\"].sents)[0])","pos":4,"type":"cell"}
{"cell_type":"code","id":"92e65f","input":"print (error_analysis, [(t, t.ent_type_) for t in error_analysis])","pos":12,"type":"cell"}
{"cell_type":"code","id":"9d1f63","input":"##TODO print the ratio of capitalized tokens not being a named entity and not being the first token in a sentence\n# e.g. \"The dog barks\" = 0; 3 tokens, \"The\" is capitalized but the starting token of a sentence, no other tokens are capitalized.\n\n##TODO print the ratio of capitalized tokens not being part of a named entity span\n# e.g. \"The dog barks\" = 1/3; 3 tokens, only \"The\" is capitalized\n\nnum_capitalized, n = 0, 0\n\n\nfor doc in dfs[\"tokenized\"]:\n    for sent in doc.sents:\n        for i, token in enumerate(sent):\n            if i == 0:\n                continue\n            if not token.ent_type_:\n                if token.text[0].isupper():\n                    num_capitalized += 1\n                    error_analysis = sent # keep one sentence where we have a capitalized token which is not the start of the sentence\n                    \n                n += 1\nprint (num_capitalized / n)","pos":11,"type":"cell"}
{"cell_type":"code","id":"b42e6c","input":"#Import the AG news dataset (same as hw01)\n#Download them from here \n#!wget https://raw.githubusercontent.com/mhjabreel/CharCnn_Keras/master/data/ag_news_csv/train.csv\n\nimport pandas as pd\nimport nltk\ndf = pd.read_csv('train.csv')\n\ndf.columns = [\"label\", \"title\", \"lead\"]\nlabel_map = {1:\"world\", 2:\"sport\", 3:\"business\", 4:\"sci/tech\"}\ndef replace_label(x):\n\treturn label_map[x]\ndf[\"label\"] = df[\"label\"].apply(replace_label) \ndf[\"text\"] = df[\"title\"] + \" \" + df[\"lead\"]\ndf.head()","pos":2,"type":"cell"}
{"cell_type":"code","id":"bcbfe7","input":"from sklearn.feature_extraction.text import TfidfVectorizer\ntfidf = TfidfVectorizer(min_df=0.01, \n                        max_df=0.9,  \n                        max_features=1000,\n                        stop_words='english',\n                        use_idf=True, # the new piece\n                        ngram_range=(1,2))\n\nfrom wordcloud import WordCloud\nimport matplotlib.pyplot as plt\nimport numpy as np\n\ndfs[\"input_TFIDF\"] = dfs[\"preprocessed\"].apply(lambda x: \" \".join(x))\nX_tfidf = tfidf.fit_transform(dfs[\"input_TFIDF\"])\nX_tfidf.shape\n\n##TODO using the whole sample, produce a world cloud with bigrams for each label using tfidf frequencies\nvocab = tfidf.get_feature_names()\nprint (vocab[:10])\n\ntotal_freqs = list(np.array(X_tfidf.sum(axis=0))[0])\nfdict = dict(zip(vocab,total_freqs))\n# generate word cloud of words with highest counts\nwordcloud = WordCloud().generate_from_frequencies(fdict) \nplt.clf()\nplt.imshow(wordcloud, interpolation='bilinear') \nplt.axis(\"off\") \nplt.show()\n","pos":15,"type":"cell"}
{"cell_type":"code","id":"d4f4e1","input":"##TODO print the first 20 noun chuncks in your sample corpus (doc.noun_chunks)\ncounter = 1\nfor doc in dfs[\"tokenized\"]:\n    for chunk in doc.noun_chunks:\n        if counter > 20:\n            break\n        print (counter, chunk)\n        counter += 1\n","pos":7,"type":"cell"}
{"cell_type":"code","id":"e5886d","input":"##TODO print the ratio of tokens being part of a named entity span starting with a capital letter (doc.ents)\n\nnum_capitalized, n = 0, 0\nfor doc in dfs[\"tokenized\"]:\n    for token in doc.ents:\n        if token.text[0].isupper():\n            num_capitalized += 1\n        n += 1\nprint (num_capitalized / n)","pos":9,"type":"cell"}
{"cell_type":"code","id":"e68954","input":"from sklearn.feature_selection import SelectKBest, f_classif, chi2\n\n\n##TODO compute the number of words per document (excluding stopwords)\n##TODO get the most predictive features of the number of words per document using first f_class and then chi2\nimport numpy as np\nvocab = tfidf.get_feature_names()\n\n# f_class\nY = dfs['label']\nselect = SelectKBest(f_classif, k=10)\nX_new = select.fit_transform(X_tfidf, Y)\nprint ([vocab[i] for i in np.argsort(select.scores_)[:10]])\n\n# chi2\nselect = SelectKBest(chi2, k=10)\nX_new = select.fit_transform(X_tfidf, Y)\nprint ([vocab[i] for i in np.argsort(select.scores_)[:10]])\n\n","pos":19,"type":"cell"}
{"cell_type":"code","id":"f4ece8","input":"# # we use distilbert tokenizer\nfrom transformers import DistilBertTokenizerFast\n\n# let's instantiate a tokenizer\ntokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')\n\n##TODO tokenize the sentences in the sampled dataframe (dfs) using the DisilBertTokenizer\n\ndfs[\"huggingface_tokenizer\"] = dfs[\"text\"].apply(lambda x: tokenizer.tokenize(x))\n\n##TODO what is the type/token ratio from this tokenizer (number_of_unqiue_token_types/number_of_tokens)?\n\ntokens, types = 0, set()\nfor doc in dfs[\"huggingface_tokenizer\"]:\n    for token in doc:\n        tokens += 1\n        types.add(token)\n        \nprint (len(types) / tokens)\n\n##TODO what is the amount of subword tokens returned by the huggingface tokenizer? hint: each subword token starts with \"#\"\nnum_subwords = 0\nfor doc in dfs[\"huggingface_tokenizer\"]:\n    for token in doc:\n        if token.startswith(\"#\"):\n            num_subwords += 1\n            \nprint (\"number of subwords:\", num_subwords, \"number of words:\", tokens)","pos":22,"type":"cell"}
{"cell_type":"markdown","id":"0525d4","input":"Give an example of a capitalized token in the data which is neither a named entity nor at the start of a sentence. What could be the reason the token is capitalized (one sentence)?","pos":13,"type":"cell"}
{"cell_type":"markdown","id":"097d17","input":"# HW02: Tokenization","pos":0,"type":"cell"}
{"cell_type":"markdown","id":"193388","input":"### Noun Chunks","pos":6,"type":"cell"}
{"cell_type":"markdown","id":"35fcb8","input":"## Supervised Feature Selection","pos":18,"type":"cell"}
{"cell_type":"markdown","id":"55bf1d","input":"## Hash Vectorizer","pos":16,"type":"cell"}
{"cell_type":"markdown","id":"5a50b8","input":"Remember that these homework work as a completion grade. **You can skip one section without losing credit.**","pos":1,"type":"cell"}
{"cell_type":"markdown","id":"96f7ba","input":"### Named Entities\n\nLet's compute the ratio of named entities starting with a capital letter, e.g. if we have \"University of Chicago\" as a NE, \"University\" and \"Chicago\" are capitalized, \"of\" is not, thus the ratio is 2/3.","pos":8,"type":"cell"}
{"cell_type":"markdown","id":"d0ed71","input":"## Huggingface Tokenizers","pos":21,"type":"cell"}
{"cell_type":"markdown","id":"e181ed","input":"Are the results different? What could be a reason for this? ","pos":20,"type":"cell"}
{"cell_type":"markdown","id":"f102ad","input":"## Preprocess Text","pos":3,"type":"cell"}
{"cell_type":"markdown","id":"faf7fe","input":"## Term Frequencies","pos":14,"type":"cell"}
{"id":0,"time":1620469171892,"type":"user"}
{"last_load":1620469172108,"type":"file"}