{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# HW02: Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Remember that these homework work as a completion grade. **You can skip one section without losing credit.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "#Import the AG news dataset (same as hw01)\n",
    "#Download them from here \n",
    "#!wget https://raw.githubusercontent.com/mhjabreel/CharCnn_Keras/master/data/ag_news_csv/train.csv\n",
    "\n",
    "import pandas as pd\n",
    "import nltk\n",
    "df = pd.read_csv('train.csv')\n",
    "\n",
    "df.columns = [\"label\", \"title\", \"lead\"]\n",
    "label_map = {1:\"world\", 2:\"sport\", 3:\"business\", 4:\"sci/tech\"}\n",
    "def replace_label(x):\n",
    "\treturn label_map[x]\n",
    "df[\"label\"] = df[\"label\"].apply(replace_label) \n",
    "df[\"text\"] = df[\"title\"] + \" \" + df[\"lead\"]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Preprocess Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "import spacy\n",
    "dfs = df.sample(50)\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "##TODO use spacy to split the documents in the sampled dataframe (dfs) in sentences and tokens\n",
    "\n",
    "dfs[\"tokenized\"] = dfs[\"text\"].apply(lambda x: nlp(x))\n",
    "\n",
    "##TODO print the first sentence of the first document in your sample\n",
    "\n",
    "print (list(dfs.iloc[0][\"tokenized\"].sents)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "##TODO create a new column with tokens in lowercase (x.lower()), without punctuation tokens (x.is_punct) nor stopwords (x.is_stop)\n",
    "\n",
    "def tokenize(x):\n",
    "    return [w.lemma_.lower() for w in nlp(x) if not w.is_stop and not w.is_punct and not w.is_digit]\n",
    "dfs[\"preprocessed\"] = dfs[\"text\"].apply(lambda x: tokenize(x))\n",
    "\n",
    "##TODO print the tokens (x.lemma_) and the tags (x.tag_ ) of the first sentence of the first document (doc.sents)\n",
    "for sent in dfs.iloc[0][\"tokenized\"].sents:\n",
    "    for token in sent:\n",
    "        print (token.lemma_, token.tag_)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Noun Chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "##TODO print the first 20 noun chuncks in your sample corpus (doc.noun_chunks)\n",
    "counter = 1\n",
    "for doc in dfs[\"tokenized\"]:\n",
    "    for chunk in doc.noun_chunks:\n",
    "        if counter > 20:\n",
    "            break\n",
    "        print (counter, chunk)\n",
    "        counter += 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Named Entities\n",
    "\n",
    "Let's compute the ratio of named entities starting with a capital letter, e.g. if we have \"University of Chicago\" as a NE, \"University\" and \"Chicago\" are capitalized, \"of\" is not, thus the ratio is 2/3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "##TODO print the ratio of tokens being part of a named entity span starting with a capital letter (doc.ents)\n",
    "\n",
    "num_capitalized, n = 0, 0\n",
    "for doc in dfs[\"tokenized\"]:\n",
    "    for token in doc.ents:\n",
    "        if token.text[0].isupper():\n",
    "            num_capitalized += 1\n",
    "        n += 1\n",
    "print (num_capitalized / n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "##TODO print the ratio of capitalized tokens not being part of a named entity span\n",
    "# e.g. \"The dog barks\" = 1/3; 3 tokens, only \"The\" is capitalized\n",
    "\n",
    "num_capitalized, n = 0, 0\n",
    "\n",
    "for doc in dfs[\"tokenized\"]:\n",
    "    for token in doc:\n",
    "        if not token.ent_type_:\n",
    "            if token.text[0].isupper():\n",
    "                num_capitalized += 1\n",
    "            n += 1\n",
    "print (num_capitalized / n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "##TODO print the ratio of capitalized tokens not being a named entity and not being the first token in a sentence\n",
    "# e.g. \"The dog barks\" = 0; 3 tokens, \"The\" is capitalized but the starting token of a sentence, no other tokens are capitalized.\n",
    "\n",
    "##TODO print the ratio of capitalized tokens not being part of a named entity span\n",
    "# e.g. \"The dog barks\" = 1/3; 3 tokens, only \"The\" is capitalized\n",
    "\n",
    "num_capitalized, n = 0, 0\n",
    "\n",
    "\n",
    "for doc in dfs[\"tokenized\"]:\n",
    "    for sent in doc.sents:\n",
    "        for i, token in enumerate(sent):\n",
    "            if i == 0:\n",
    "                continue\n",
    "            if not token.ent_type_:\n",
    "                if token.text[0].isupper():\n",
    "                    num_capitalized += 1\n",
    "                    error_analysis = sent # keep one sentence where we have a capitalized token which is not the start of the sentence\n",
    "                    \n",
    "                n += 1\n",
    "print (num_capitalized / n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "print (error_analysis, [(t, t.ent_type_) for t in error_analysis])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Give an example of a capitalized token in the data which is neither a named entity nor at the start of a sentence. What could be the reason the token is capitalized (one sentence)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Term Frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf = TfidfVectorizer(min_df=0.01, \n",
    "                        max_df=0.9,  \n",
    "                        max_features=1000,\n",
    "                        stop_words='english',\n",
    "                        use_idf=True, # the new piece\n",
    "                        ngram_range=(1,2))\n",
    "\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "dfs[\"input_TFIDF\"] = dfs[\"preprocessed\"].apply(lambda x: \" \".join(x))\n",
    "X_tfidf = tfidf.fit_transform(dfs[\"input_TFIDF\"])\n",
    "X_tfidf.shape\n",
    "\n",
    "##TODO using the whole sample, produce a world cloud with bigrams for each label using tfidf frequencies\n",
    "vocab = tfidf.get_feature_names()\n",
    "print (vocab[:10])\n",
    "\n",
    "total_freqs = list(np.array(X_tfidf.sum(axis=0))[0])\n",
    "fdict = dict(zip(vocab,total_freqs))\n",
    "# generate word cloud of words with highest counts\n",
    "wordcloud = WordCloud().generate_from_frequencies(fdict) \n",
    "plt.clf()\n",
    "plt.imshow(wordcloud, interpolation='bilinear') \n",
    "plt.axis(\"off\") \n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Hash Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "\n",
    "hv = HashingVectorizer(n_features=5000)\n",
    "\n",
    "##TODO print the first 10 features produced by the hash vectorizer\n",
    "from eli5.sklearn import InvertableHashingVectorizer\n",
    "X_hash = hv.fit_transform(dfs['input_TFIDF'])\n",
    "ivec = InvertableHashingVectorizer(hv)\n",
    "inverted_hv = ivec.fit(dfs['input_TFIDF'])\n",
    "\n",
    "#print ([i for i in inverted_hv.get_feature_names()[:10]])\n",
    "counter = 1\n",
    "for i in inverted_hv.get_feature_names():\n",
    "    if counter > 10:\n",
    "        break\n",
    "    if isinstance(i, list):\n",
    "        print (i)\n",
    "        counter += 1\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Supervised Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "from sklearn.feature_selection import SelectKBest, f_classif, chi2\n",
    "\n",
    "\n",
    "##TODO compute the number of words per document (excluding stopwords)\n",
    "##TODO get the most predictive features of the number of words per document using first f_class and then chi2\n",
    "import numpy as np\n",
    "vocab = tfidf.get_feature_names()\n",
    "\n",
    "# f_class\n",
    "Y = dfs['label']\n",
    "select = SelectKBest(f_classif, k=10)\n",
    "X_new = select.fit_transform(X_tfidf, Y)\n",
    "print ([vocab[i] for i in np.argsort(select.scores_)[:10]])\n",
    "\n",
    "# chi2\n",
    "select = SelectKBest(chi2, k=10)\n",
    "X_new = select.fit_transform(X_tfidf, Y)\n",
    "print ([vocab[i] for i in np.argsort(select.scores_)[:10]])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Are the results different? What could be a reason for this? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Huggingface Tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "# # we use distilbert tokenizer\n",
    "from transformers import DistilBertTokenizerFast\n",
    "\n",
    "# let's instantiate a tokenizer\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "##TODO tokenize the sentences in the sampled dataframe (dfs) using the DisilBertTokenizer\n",
    "\n",
    "dfs[\"huggingface_tokenizer\"] = dfs[\"text\"].apply(lambda x: tokenizer.tokenize(x))\n",
    "\n",
    "##TODO what is the type/token ratio from this tokenizer (number_of_unqiue_token_types/number_of_tokens)?\n",
    "\n",
    "tokens, types = 0, set()\n",
    "for doc in dfs[\"huggingface_tokenizer\"]:\n",
    "    for token in doc:\n",
    "        tokens += 1\n",
    "        types.add(token)\n",
    "        \n",
    "print (len(types) / tokens)\n",
    "\n",
    "##TODO what is the amount of subword tokens returned by the huggingface tokenizer? hint: each subword token starts with \"#\"\n",
    "num_subwords = 0\n",
    "for doc in dfs[\"huggingface_tokenizer\"]:\n",
    "    for token in doc:\n",
    "        if token.startswith(\"#\"):\n",
    "            num_subwords += 1\n",
    "            \n",
    "print (\"number of subwords:\", num_subwords, \"number of words:\", tokens)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (system-wide)",
   "language": "python",
   "metadata": {
    "cocalc": {
     "description": "Python 3 programming language",
     "priority": 100,
     "url": "https://www.python.org/"
    }
   },
   "name": "python3",
   "resource_dir": "/ext/jupyter/kernels/python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}